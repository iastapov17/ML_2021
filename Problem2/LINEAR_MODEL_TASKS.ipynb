{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T07:26:47.648991Z",
     "start_time": "2019-10-18T07:26:46.395720Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Дополнительные функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T07:26:47.655990Z",
     "start_time": "2019-10-18T07:26:47.650982Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def time_delta(func, *args, **kwargs): \n",
    "    start_time = time.time()\n",
    "    func(*args, **kwargs)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    return end_time-start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T07:26:47.761930Z",
     "start_time": "2019-10-18T07:26:47.658977Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def merge_two_dicts(dict1, dict2):\n",
    "    dict3 = defaultdict(list)\n",
    "    items = list(dict1.items())+list(dict2.items())\n",
    "\n",
    "    for k, v in items:\n",
    "        for item in v:\n",
    "            dict3[k].append(item)\n",
    "    return dict3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T07:26:47.935818Z",
     "start_time": "2019-10-18T07:26:47.930821Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def read_task_1(file_name, header, sep='|'):\n",
    "    result = {}\n",
    "    data = np.genfromtxt(file_name, delimiter=sep, skip_header=1)\n",
    "    for i, name in enumerate(header):\n",
    "        result[name] = data[:,i]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T07:26:48.495498Z",
     "start_time": "2019-10-18T07:26:48.443538Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def read_task_2_3(file_name, sep='|', test_size=.3, bias=True):\n",
    "    data = np.genfromtxt(file_name, delimiter=sep, skip_header=1)\n",
    "    \n",
    "    train_idx = np.random.choice(range(data.shape[0]), size=int(data.shape[0]*(1 - test_size)), replace = False)\n",
    "    test_idx  = np.array([i for i in range(data.shape[0]) if i not in train_idx])\n",
    "    \n",
    "    if bias:\n",
    "        X_train = np.hstack((data[train_idx,:-1], \n",
    "                             np.ones((train_idx.shape[0], 1))\n",
    "                            ))\n",
    "        X_test =  np.hstack((data[test_idx,:-1],\n",
    "                         np.ones((test_idx.shape[0], 1))\n",
    "                        ))\n",
    "    else:\n",
    "        X_train = data[train_idx,:-1]\n",
    "        X_test = data[test_idx,:-1]\n",
    "        \n",
    "    y_train = data[train_idx,-1] \n",
    "    \n",
    "   \n",
    "    y_test = data[test_idx,-1]\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T07:32:12.744073Z",
     "start_time": "2019-10-18T07:32:12.739080Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def plot_correlation_matrix(corr_mtrx):\n",
    "    f = plt.figure(figsize=(8, 8))\n",
    "    plt.matshow(corr_mtrx, fignum=f.number)\n",
    "    plt.xticks(range(corr_mtrx.shape[1]), fontsize=9, rotation=45)\n",
    "    plt.yticks(range(corr_mtrx.shape[1]), fontsize=9)\n",
    "    cb = plt.colorbar()\n",
    "    plt.title('Correlation Matrix', fontsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# План\n",
    "\n",
    "__Вы научитесь:__\n",
    "    1. Решать задачу линейной регрессии аналитически\n",
    "    2. Понимать как работают классические метрики оценки качества линейной регрессии\n",
    "    3. Разбираться что такое полиномиальная регрессия и как сделать линейную регрессию не совсем линейной (как так выходит?)\n",
    "    4. Понимать что такое значимость коэффициентов  и зачем она нужна (Difference in Differences?)\n",
    "    5. Реализовывать градиентный и стохастический градиентый спуск для задачи линейной регрессии\n",
    "    6. Понимать как работает регуляризация \n",
    "    7. Превращать линейный регрессор в линейный классификатор\n",
    "    \n",
    "__Важно:__ Ноутбук стоит проходить последовательно. Каждое новое задание использует результат и функции предыдущих шагов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Одномерный случай"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>__Загрузка данных__</font>\n",
    "\n",
    "Вы получите __словарь__ со следубщими ключами:\n",
    "- __x_train__:  значение некоторого показателя\n",
    "- __y_train__:  целевое значение некоторой функции\n",
    "- __mainfold__: идеальное значение предсказания\n",
    "\n",
    "Значения словаря: одномерный numpy массив.\n",
    "\n",
    "P.S. Да, тестовых данных тут нет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T07:26:51.220197Z",
     "start_time": "2019-10-18T07:26:51.213202Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "data = read_task_1('./task_1.csv', header=['x_train', 'y_train', 'manifold'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3, color=\"green\">__Задание №0.1: визуализация__</font>\n",
    "\n",
    "Визуализируйте зависимость _y_train_ от _x_train_ и значения _manifold_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T07:27:59.367573Z",
     "start_time": "2019-10-18T07:27:59.365590Z"
    }
   },
   "outputs": [],
   "source": [
    "margin = 0.3\n",
    "plt.figure(figsize=(20,10))\n",
    "\n",
    "plt.scatter(data['x_train'], data['manifold'], 40, 'g', 'o', alpha=0.8, label='manifold')\n",
    "plt.scatter(data['x_train'], data['y_train'], 40, 'b', 'o', alpha=0.8, label='data')\n",
    "\n",
    "plt.xlim(data['x_train'].min() - margin, data['x_train'].max() + margin)\n",
    "plt.ylim(data['y_train'].min() - margin, data['y_train'].max() + margin)\n",
    "\n",
    "plt.legend(loc='upper left', prop={'size': 20})\n",
    "plt.title('True manifold and noised data')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Метод наименьших квадратов (Ordinary Least Squares)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<font size=3>__Немного теории__</font>\n",
    "\n",
    "Линейная регрессия позволяет прогнозировать значения количественного показателя в виде ___линейной комбинацией признаков___ с некоторыми весами. Поиск ___весов___ является целью обучения данного алгоритма, что достигается при помощи ___минимизации___ некоторого ___функционала ошибки___.\n",
    "\n",
    "<font size=2.3>__Как строится предсказание?__</font>\n",
    "\n",
    "Функция зависимости $y$ от $x$ будет иметь следующий вид $y_i=\\sum_{j=1}^m{w_jx_{ij}}+w_{0}+\\epsilon_i$, что и является ___линейной комбинацией признаков___. Обобщить данное выражение можно с помощью добавления фиктивной переменной $x_0=1$ (bais). И тогда в матричном виде получаем:\n",
    "\n",
    "<font size=4>$$y=Xw+\\epsilon$$</font>\n",
    "где:\n",
    "- $w$ $-$ оценка коэффициентов регрессии\n",
    "- $X$ $-$ матрица наблюдений состоящая из $n$ строк и $m+1$ столбцов (вопрос в студию: почему +1?)\n",
    "- $\\epsilon$ $-$ случайная ошибка модели\n",
    "\n",
    "<font size=2.3>__Как найти \"подходящие\" веса?__</font>\n",
    "\n",
    "\n",
    "Метод наименьших квадратов один из способов их поиска, в рамках которого используется __среднеквадратичная ошибка__\n",
    "\n",
    "<font size=4>\\begin{array}{rcl}\\mathcal{L}\\left( X, {y}, {w} \\right) &=& \\frac{1}{2n} \\sum_{i=1}^n \\left(y_i - {w}^\\text{T} {x}_i\\right)^2 \\\\\n",
    "&=& \\frac{1}{2n} \\left\\| {y} -  X {w} \\right\\|_2^2 \\\\\n",
    "&=& \\frac{1}{2n} \\left({y} -  X {w}\\right)^\\text{T} \\left({y} -  X {w}\\right)\n",
    "\\end{array}</font>\n",
    "\n",
    "\n",
    "Для решения задачи оптимизации необходимо взять производную по весам $w$ и прировнять ее к нулю:\n",
    "\n",
    "\\begin{array}{rcl} \\frac{\\partial \\mathcal{L}}{\\partial {w}} &=& \\frac{1}{2n} \\left( {y}^{\\text{T}} {y} -2{y}^{\\text{T}} {X} {w} + {w}^{\\text{T}} {X}^{\\text{T}} {X} {w}\\right) \\\\\n",
    "&=& \\frac{1}{2n} \\left(-2 {X}^{\\text{T}} {y} + 2{X}^{\\text{T}} {X} {w}\\right)\n",
    "\\end{array}\n",
    "\n",
    "\n",
    "\\begin{array}{rcl} \\frac{\\partial \\mathcal{L}}{\\partial {w}} = 0 &\\Leftrightarrow& \\frac{1}{2n} \\left(-2 {X}^{\\text{T}} {y} + 2{X}^{\\text{T}} {X} {w}\\right) = 0 \\\\\n",
    "&\\Leftrightarrow& -{X}^{\\text{T}} {y} + {X}^{\\text{T}} {X} {w} = 0 \\\\\n",
    "&\\Leftrightarrow& {X}^{\\text{T}} {X} {w} = {X}^{\\text{T}} {y} \\\\\n",
    "\\end{array} \n",
    "\n",
    "<font size=4>$$ {w} = \\left({X}^{\\text{T}} {X}\\right)^{-1} {X}^{\\text{T}} {y}$$ </font>\n",
    "\n",
    "Решение, полученные МНК, имеет наименьшую дисперсию среди всех линейных и несмещенных оценок, при условии выполнения \n",
    "[теоремы Гаусса - Маркова](https://ru.wikipedia.org/wiki/Теорема_Гаусса_—_Маркова).\n",
    "\n",
    "Также эту задачу можно решать численно с помощью градиентного спуска, об этом речь пойдет дальше.\n",
    "\n",
    "<font size=3 color=\"green\">__Задание №1: Аналитическое решение МНК__</font>\n",
    "\n",
    "__В данном задании вам необходимо реализовать 2 функции:__\n",
    "- __least_squares_weights__: в рамках которой необходимо реализовать МНК подход к поиску коэфициентов регрессии. Функция должна возвращать одномерный numpy массив длины $m$, где $m$ $-$ число признаков матрицы $x$\n",
    "- __least_squares_predict__: Функция должна возвращать одномерный numpy массив длины $n$, где $n$ $-$ число объектов матрицы $x$. Принимает на вход матрицу объекты-признаки $x$ и веса $w$. \n",
    "\n",
    "__!!ВАЖНО!!__: \n",
    "- не используйте в этих функциях циклы $-$ тогда она будет вычислительно неэффективной\n",
    "- не изменяйте значения X_train и Y_train, иначе проверка не отработает (необходимо только в этом задании).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T06:48:31.695928Z",
     "start_time": "2019-10-18T06:48:31.692930Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = np.array([data['x_train']]).T\n",
    "Y_train = data['y_train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T07:29:01.082705Z",
     "start_time": "2019-10-18T07:29:01.079715Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def least_squares_weights(x, y):\n",
    "    w = np.matmul(np.matmul(np.linalg.inv(np.matmul(x.T, x)), x.T), y)\n",
    "    return w\n",
    "\n",
    "W = least_squares_weights(X_train, Y_train)\n",
    "\n",
    "# TESTS\n",
    "assert type(W) == np.ndarray, 'Возвращается не корректный тип'\n",
    "assert W.shape == (1,), 'Проверьте размерность выходящего вектора'\n",
    "assert round(W[0], 3) == 0.493, 'Не верно реализован расчёт весов'\n",
    "assert time_delta(least_squares_weights, X_train, Y_train) < 1.5, 'Функция работает слишком долго, возможно вы используете лишние циклы'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T06:48:32.860259Z",
     "start_time": "2019-10-18T06:48:32.854261Z"
    }
   },
   "outputs": [],
   "source": [
    "def least_squares_predict(w, x):\n",
    "    result = np.matmul(x, w)\n",
    "    return result\n",
    "\n",
    "Y_train_predict = least_squares_predict(W, X_train)\n",
    "\n",
    "# TESTS\n",
    "assert type(Y_train) == np.ndarray, 'Возвращается не корректный тип'\n",
    "assert Y_train_predict.shape == (500,), 'Проверьте размерность выходящего вектора'\n",
    "assert round(Y_train_predict[123], 3) == -3.706, 'Не верно реализован расчёт весов'\n",
    "assert time_delta(least_squares_predict, W, X_train) < 1.5, 'Функция работает слишком долго, возможно вы используете лишние циклы'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3 >__Немного теории__</font>\n",
    "\n",
    "Мы разберем __основные метрики качества и функционалы потерь__ задачи регрессии. Далее все будем называть __метриками__.\n",
    "\n",
    "__Метрики необходимы для:__\n",
    "- поиска оптимального решения\n",
    "- оценки качества работы модели\n",
    "- сравнения моделей\n",
    "- интерпретации результатов\n",
    "\n",
    "\n",
    "\n",
    "<table border=\"1\" width=\"100%\">\n",
    " <tr>\n",
    "    <td width=\"50%\"> \n",
    "        <p> <font size=3> $MAE= \\frac{1}{n} \\sum_{i=1}^n |y_i-\\hat{y_i}|$  </font> </p> \n",
    "        <p> \n",
    "            <font size=2> \n",
    "                <span style=\"color:green; margin-left:2em\">$ +$ </span>\n",
    "                легко интерпретировать \n",
    "            </font><br/>\n",
    "            <font size=2> \n",
    "                <span style=\"color:green; margin-left:2em\">$ +$ </span>\n",
    "                единицы измерения таргета и метрики – эквивалентны\n",
    "            </font><br/>\n",
    "            <font size=2> \n",
    "                <span style=\"color:green; margin-left:2em\">$ +$ </span>\n",
    "                устойчива к выбросам \n",
    "            </font><br/>\n",
    "            <font size=2> \n",
    "                <span style=\"color:red; margin-left:2em\">$ -$ </span>\n",
    "                не ограничена сверху \n",
    "            </font><br/>\n",
    "            <font size=2> \n",
    "                <span style=\"color:red; margin-left:2em\">$ -$ </span>\n",
    "                не дифференцируема в таком виде \n",
    "            </font>\n",
    "        </p><br/>\n",
    "        <p> <font size=3> $MSE= \\frac{1}{n} \\sum_{i=1}^n (y_i-\\hat{y_i})^2$  </font> </p> \n",
    "        <p> \n",
    "            <font size=2> \n",
    "                <span style=\"color:green; margin-left:2em\">$ +$ </span>\n",
    "                дифференцируема\n",
    "            </font><br/>\n",
    "            <font size=2> \n",
    "                <span style=\"color:green; margin-left:2em\">$ +$ </span>\n",
    "                чувствительна к выбросам\n",
    "            </font><br/>\n",
    "            <font size=2> \n",
    "                <span style=\"color:red; margin-left:2em\">$ -$ </span>\n",
    "                не ограничена сверху \n",
    "            </font><br/>\n",
    "            <font size=2> \n",
    "                <span style=\"color:red; margin-left:2em\">$ -$ </span>\n",
    "                сложно интерпретировать \n",
    "            </font><br/>\n",
    "        </p><br/>\n",
    "        <p> <font size=3> $RMSE=\\sqrt{\\frac{1}{n} \\sum_{i=1}^n (y_i-\\hat{y_i})^2} $  </font> </p> \n",
    "        <p> \n",
    "            <font size=2> \n",
    "                <span style=\"color:green; margin-left:2em\">$ +$ </span>\n",
    "                дифференцируема\n",
    "            </font><br/>\n",
    "            <font size=2> \n",
    "                <span style=\"color:red; margin-left:2em\">$ -$ </span>\n",
    "                не ограничена сверху \n",
    "            </font><br/>\n",
    "            <font size=2> \n",
    "                <span style=\"color:red; margin-left:2em\">$ -$ </span>\n",
    "                еще сложнее интерпретировать \n",
    "            </font>\n",
    "        <p>Основное различие $RMSE$ и $MAE$ заключается в том, что минимизация $RMSE$ сремиться к средней оценке, а  $MAE$ к медиане</p><br/><p>$y$ $-$ истинное значение; $\\hat{y}$ $-$ предсказанное значение</p>\n",
    "        </p><br/>\n",
    "     </td>\n",
    "    <td><img src=\"./errors_2.png\" align=\"right\"></td>\n",
    " </tr>\n",
    "</table>\n",
    "\n",
    "Две попытки __объединить__ положительные __свойства метрик__:\n",
    "- [Функция потерь Хьюбера](https://en.wikipedia.org/wiki/Huber_loss): \n",
    "    * ведет себя как $MSE$ на ошибках меньше $\\sigma$ и как $MAE$ в противном случае. __Что не дает \"взрываться\" метрике на больших значениях и \"жестко\" реагирует на маленькие остатки__\n",
    "    * все так же не удобно дифференцировать\n",
    "- [Log-Cosh Loss](https://heartbeat.fritz.ai/5-regression-loss-functions-all-machine-learners-should-know-4fb140e9d4b0):\n",
    "    * принимает меньшие значения по сравнению с функцией потерь Хьюбера на всем множестве\n",
    "    * __дважды дифференцируема__, что необходимо в некоторых методах численной оптимизации.\n",
    "________________________\n",
    "__Проблема:__ все предыдущие метрики __не подходят для оценки конкретного решения__ а только для сравнения моделей, что следует из того что __функции не ограничены сверху.__ \n",
    "\n",
    "Эта проблема так же решается посредством доработок $MAE$: [MAPE](https://en.wikipedia.org/wiki/Mean_absolute_percentage_error) и [SMAPE](https://en.wikipedia.org/wiki/Symmetric_mean_absolute_percentage_error). Однако, подробно рассмотрим метрику $R^2$\n",
    "и мотивы появления $R^2_{adj}$.\n",
    "\n",
    "$$R^2=1-\\frac{\\sum_{i=1}^n (y_i-\\hat{y_i})^2}{\\sum_{i=1}^n (y_i-\\bar{y_i})^2}$$\n",
    "\n",
    "Данная метрика обладает следующими свойствами:\n",
    "\n",
    "<font size=2> \n",
    "    <span style=\"color:green; margin-left:2em\">$ +$ </span>\n",
    "     ограничена сверху и снизу, что позволяет дать оценку конкретному решению без сравнения с другими\n",
    "</font><br/>\n",
    "<font size=2> \n",
    "    <span style=\"color:green; margin-left:2em\">$ +$ </span>\n",
    "     хорошо интерпретируется: на сколько наша модель лучше, чем константное решение\n",
    "</font><br/>\n",
    "<font size=2> \n",
    "    <span style=\"color:red; margin-left:2em\">$ -$ </span>\n",
    "     не диффиренцируема\n",
    "</font><br/>\n",
    "<font size=2> \n",
    "    <span style=\"color:red; margin-left:2em\">$ -$ </span>\n",
    "     значение метрики растет (не уменьшается) при добавлении новых переменных, даже если они никак не объясняют целевой показатель. Поэтому нельзя сравнивать модели с разным количеством признаков. Данная проблема решается с помощью $R^2_{adj}$\n",
    "</font>\n",
    "\n",
    "$$R^2_{adj}=1-(1-R^2)\\frac{n-1}{n-p-1}$$\n",
    "\n",
    "где $p -$ число признаков, а $n -$ количество объектов\n",
    "\n",
    "__Рекомендации:__ использовать $R^2_{adj}$ для сравнения моделей и оценки количества переменных, а $R^2$ для оценки качества на новых объектах\n",
    "\n",
    "<font size=3 color=\"green\">__Задание №2: Метрики качества__</font>\n",
    "\n",
    "__В данном задании вам необходимо реализовать 4 основные метрики:__ $MAE; MSE; RMSE; R^2$\n",
    "\n",
    "Каждая из метрик принимает на вход 2 одномерных numpy массива:\n",
    "- __y_true__ - истинные значения \n",
    "- __y_predict__ - предсказанные значения\n",
    "\n",
    "Каждая из функций должна возврашать одно число \n",
    "\n",
    "__!!ВАЖНО!!__: не используйте в этих функциях циклы $-$ тогда она будет вычислительно неэффективной."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T06:48:34.210482Z",
     "start_time": "2019-10-18T06:48:34.197494Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def MAError(y_true, y_predict):\n",
    "    error = np.mean(np.abs(y_true - y_predict))\n",
    "    return error\n",
    "\n",
    "def MSError(y_true, y_predict):\n",
    "    error = np.mean((y_true - y_predict) ** 2)\n",
    "    return error\n",
    "\n",
    "def RMSError(y_true, y_predict):\n",
    "    error = np.sqrt(np.mean((y_true - y_predict) ** 2))\n",
    "    return error\n",
    "\n",
    "def R_square(y_true, y_predict):\n",
    "    error = 1 - ((y_true - y_predict) ** 2).sum() / ((y_true - np.average(y_true)) ** 2).sum()\n",
    "    return error\n",
    "\n",
    "def all_metric(y_true, y_predict):\n",
    "    d = {}\n",
    "    d['MAE'] = [MAError(y_true, y_predict)] \n",
    "    d['MSE'] = [MSError(y_true, y_predict)] \n",
    "    d['RMSE'] = [RMSError(y_true, y_predict)]\n",
    "    d['R_square'] = [R_square(y_true, y_predict)]\n",
    "    return d\n",
    "\n",
    "# TESTS\n",
    "assert round(MAError(y_true=Y_train, y_predict=Y_train_predict), 3) == 1.274, 'Не верно реализован расчёт'\n",
    "assert time_delta(MAError, Y_train, Y_train_predict) < 1.5, 'Функция работает слишком долго, возможно вы используете лишние циклы'\n",
    "\n",
    "assert round(MSError(y_true=Y_train, y_predict=Y_train_predict), 3) == 2.22, 'Не верно реализован расчёт'\n",
    "assert time_delta(MSError, Y_train, Y_train_predict) < 1.5, 'Функция работает слишком долго, возможно вы используете лишние циклы'\n",
    "\n",
    "assert round(RMSError(y_true=Y_train, y_predict=Y_train_predict), 3)== 1.49, 'Не верно реализован расчёт'\n",
    "assert time_delta(RMSError, Y_train, Y_train_predict) < 1.5, 'Функция работает слишком долго, возможно вы используете лишние циклы'\n",
    "\n",
    "assert round(R_square(y_true=Y_train, y_predict=Y_train_predict), 3)== 0.843, 'Не верно реализован расчёт'\n",
    "assert time_delta(R_square, Y_train, Y_train_predict) < 1.5, 'Функция работает слишком долго, возможно вы используете лишние циклы'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3, color=\"green\">__Задание №0.2: визуализация__</font>\n",
    "\n",
    "Визуализируйте зависимость _y_train_ от _x_train_; значения _manifold_; а так же предсказанные значения для каждой точки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T07:30:26.478171Z",
     "start_time": "2019-10-18T07:30:26.475173Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "\n",
    "plt.scatter(data['x_train'], data['manifold'], 40, 'g', 'o', alpha=0.8, label='manifold')\n",
    "plt.scatter(data['x_train'], data['y_train'], 40, 'b', 'o', alpha=0.8, label='data')\n",
    "plt.scatter(data['x_train'], Y_train_predict, 40, 'r', 'o', alpha=0.8, label='pred')\n",
    "\n",
    "\n",
    "plt.legend(loc='upper right', prop={'size': 20})\n",
    "plt.title('True manifold and noised data')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T13:03:16.500775Z",
     "start_time": "2019-09-25T13:03:16.469796Z"
    }
   },
   "source": [
    "<font size=3, color=\"green\">__Задание №3: Улучшить модель__</font>\n",
    "\n",
    "Если все сделано верно, то ваш текуший результат $R^2 \\approx 0.84$, что в целом неплохо, но из графика видно, что можно лучше.\n",
    "\n",
    "_Сообщите, если это нет так_\n",
    "\n",
    "__Цель:__ Улучшить результат, оставив линейный характер предсказания. Кажется, что это можно сделать, добавив лишь один признак."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T06:48:36.669083Z",
     "start_time": "2019-10-18T06:48:36.662072Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_ = np.column_stack((np.ones(np.size(X_train)), X_train))\n",
    "W_new = least_squares_weights(X_train_, Y_train)\n",
    "Y_train_predict_new = least_squares_predict(W_new, X_train_)\n",
    "\n",
    "\n",
    "assert round(R_square(y_true=Y_train, y_predict=Y_train_predict_new), 3) >= .95, 'можно лучше'\n",
    "assert X_train_.shape[1] == 2, 'должно быть 2 признака'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Полиномиальная регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ленейная модель требует линейную зависимость между целевой и объясняющей переменной, что часто нет так. Однако можно \"скривить\" предсказание.\n",
    "\n",
    "<font size=3 >__Немного теории__</font>\n",
    "\n",
    "Что бы __увеличить сложность модели__ нам необходимо увеличить сложность признаков. В общем случае мы переходим от __линейного уровнения к полиному__:\n",
    "\n",
    "$$Y=w_0+w_1x \\Rightarrow Y=w_0+w_1x+w_2x^2$$\n",
    "\n",
    "Это __все еще линейная модель__ так как веса все еще линейны. $x^2$ всего лишь признак описывающий квадратичную зависимость $y$ от $x$.\n",
    "\n",
    "В целом, не обязательно нужно брать полиномы. Можно накладывать любую нелинейную функцию, а также функции от нескольких переменных, если у нас более одной описывающей переменной.\n",
    "\n",
    "<font size=3, color=\"green\">__Задание №4: Полиномиальная регрессия__</font>\n",
    "\n",
    "В данном задании вы увидите поведение предсказания и метрик качества в зависимости от степени полинома. Исходная функция предсказания примет следующий вид:\n",
    "   \n",
    "   $$Y=w_0+\\sum_{j=1}^nw^j_1x$$\n",
    "\n",
    "__Цель__: Реализовать __код или функцию__, которая будет вызываться в уже написанном цикле, принимать на вход размер полинома (значение из degree_variance) и значения $X$ и возвращать пространство соответствующего размера.\n",
    "\n",
    "\n",
    "__!!ВАЖНО!!__: \n",
    "- большая часть кода уже написана, не стоит ее менять\n",
    "- не забывайте $w_0$ (bais вектор), сделать это необхомо посредством добавления фиктивной переменной"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polyniom(x_train, v):\n",
    "    if x_train.shape[1] == 1:\n",
    "        return np.column_stack((np.ones(np.size(X_train)), X_train))\n",
    "    return np.column_stack([x_train] + list(x_train[:, 1] ** ind for ind in range(x_train.shape[1], v + 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T07:31:28.953931Z",
     "start_time": "2019-10-18T07:31:28.949934Z"
    }
   },
   "outputs": [],
   "source": [
    "degree_variance = range(1,24, 2) \n",
    "train_errors, test_errors = {}, {}\n",
    "\n",
    "# PLOT SET\n",
    "fig = plt.figure(figsize=(20, 25))\n",
    "colors = [plt.get_cmap('gist_rainbow')(i) for i in np.linspace(0,1,len(degree_variance))]\n",
    "\n",
    "ax1 = plt.subplot(311)\n",
    "ax1.plot(data['x_train'], data['manifold'], 'b--', alpha=0.5, label='manifold')\n",
    "ax1.scatter(data['x_train'], data['y_train'], 40, 'g', 'o', alpha=0.8, label='train_data')\n",
    "ax1.set_xlim(data['x_train'].min() - margin, data['x_train'].max() + margin)\n",
    "ax1.set_ylim(data['y_train'].min() - margin, data['y_train'].max() + margin)\n",
    "\n",
    "for i, v in enumerate(degree_variance):\n",
    " \n",
    "    # TRAIN\n",
    "    X_train = polyniom(X_train, v)\n",
    "\n",
    "    assert X_train.shape[1] == 1+v, 'Количество признаков определяется как степень полинома + basi'\n",
    "    \n",
    "    \n",
    "    Y_train = data['y_train']\n",
    "\n",
    "    # PREDICT\n",
    "    W = least_squares_weights(X_train, Y_train)\n",
    "    Y_train_predict = least_squares_predict(W, X_train)\n",
    "    \n",
    "    # PLOT SOLUTION\n",
    "    ax1.plot(data['x_train'], Y_train_predict, color = colors[i], alpha=0.8, label='degree: {}'.format(v))   \n",
    "    ax1.legend(loc='upper left', prop={'size': 12})\n",
    "    \n",
    "    # ERRORS\n",
    "    train_errors = merge_two_dicts(train_errors, all_metric(Y_train, Y_train_predict))\n",
    "\n",
    "# PLOT ERRORS\n",
    "for i, key in enumerate(train_errors.keys()):\n",
    "    plt.subplot(3,2,i+3)\n",
    "    plt.plot(degree_variance, train_errors[key], 'go-', alpha=0.5, label='train {}'.format(key));\n",
    "    plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T06:48:39.717243Z",
     "start_time": "2019-10-18T06:48:39.712245Z"
    }
   },
   "outputs": [],
   "source": [
    "assert len(train_errors['MAE']) == 12, 'степеней полинома должно быть 12 как указано в коде'\n",
    "assert sum(map(lambda i: train_errors['R_square'][i]>train_errors['R_square'][i-1], range(1, 12))) == 10, 'на каждой итерации, кроме последней, значение метрики должно расти по сравнению с предыдущей'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Многомерный случай"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее будем работать с новыми данными. Теперь у нас 15 описывающих показателей.\n",
    "\n",
    "__!!ВАЖНО!!__: Добавлять bias не нужно. В данных уже есть переменная $X_{14}=1$, вес при которой, будет отвечать за bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T06:48:41.036485Z",
     "start_time": "2019-10-18T06:48:40.726663Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "np.random.seed(0) \n",
    "\n",
    "X_train, Y_train, X_test, Y_test = read_task_2_3('./task_2.csv')\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### линейно зависимые признаки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T09:35:48.643271Z",
     "start_time": "2019-10-16T09:35:48.638259Z"
    }
   },
   "source": [
    "Воспользуемся функциями из предыдущих заданий и посчитаем веса и метрики для многомерного случая"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T06:48:42.286768Z",
     "start_time": "2019-10-18T06:48:42.271774Z"
    }
   },
   "outputs": [],
   "source": [
    "W = least_squares_weights(X_train[:,:], Y_train)\n",
    "\n",
    "Y_train_predict = least_squares_predict(W, X_train[:,:])\n",
    "Y_test_predict  = least_squares_predict(W, X_test[:,:])\n",
    "\n",
    "\n",
    "# все ОК аналитическое решение работает \n",
    "print([round(i, 2) for i in W])\n",
    "print(all_metric(Y_train, Y_train_predict))\n",
    "print(all_metric(Y_test, Y_test_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$R^2$ почти идеальный, но, кажется, что остальные метрики могут быть лучше"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Признаки не должны быть скоррелированны $-$ одно из необходимых условий для получения оптимальных несмещенных оценок c помощью МНК. Напомню, что данное требование вытекает из [теоремы Гаусса - Маркова](https://ru.wikipedia.org/wiki/Теорема_Гаусса_—_Маркова).\n",
    "\n",
    "Давайте проверим, выполняется ли это условие в нашем случае. Рассчитаем значение [корреляции Пирсона](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient) для каждой пары признаков.\n",
    "\n",
    "$$\\rho_{x_ix_j}=\\frac{\\sum_{k=1}^n(x_i-\\bar{x_i})(x_j-\\bar{x_j})}{\\sqrt{\\sum_{k=1}^n(x_i-\\bar{x_i})^2\\sum_{k=1}^n(x_j-\\bar{x_j})^2}}$$\n",
    "\n",
    "__где__,\n",
    "\n",
    "- $i, j$ $-$ индексы признаков\n",
    "- $k$ $-$ индекс объекта\n",
    "\n",
    "<font size=3, color=\"green\">__Задание №5: Корреляция__</font>\n",
    "\n",
    "Релизовать функцию __correlation_matrix__:\n",
    "\n",
    "- __вход:__ матрица объекты-признаки ($X$) размером $m\\times n$, где $m -$ количество объектов; $n -$ количество признаков\n",
    "- __выход:__ матрица, заполненная значениями корреляции Пирсона\n",
    "    - матрица квадратная\n",
    "    - симметричная матрица\n",
    "    - значение элемента выходной матрицы $a_{ij} - $ значение корреляции Пирсона $i$-ом и $j$-ом признаком \n",
    "    - корреляция не может быть расчитана, когда один из признаков константный\n",
    "   \n",
    "__!!ВАЖНО!!__: не стоит реализовывать функцию \"в лоб\" поищите ответ в numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T07:33:02.986898Z",
     "start_time": "2019-10-18T07:33:02.983885Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def correlation_matrix(x):\n",
    "    result = np.corrcoef(x)\n",
    "    return result\n",
    "\n",
    "corr_matrix = correlation_matrix((np.delete(X_train, -1, axis=1)).T) \n",
    "\n",
    "#TESTS\n",
    "assert type(corr_matrix) == np.ndarray, 'Возвращается не корректный тип'\n",
    "assert corr_matrix.shape[0] == corr_matrix.shape[1], 'Матрица не квадратная'\n",
    "assert corr_matrix.shape[0] == 14, 'Выбраны не все необходимы показатели для матрицы корреляций или вы не убрали показатель, отвечающий за bais'\n",
    "assert sum(map(lambda n: corr_matrix[n,n], range(0, corr_matrix.shape[0]))) == 14, 'Не диагональная матрица'\n",
    "assert sum(map(lambda n, k: round(corr_matrix[n,k], 3)==round(corr_matrix[k,n],3), \n",
    "                   range(0, corr_matrix.shape[0]), \n",
    "                   np.random.randint(14, size=14))), 'Матрица не симметрична'\n",
    "\n",
    "# Визуализация матрицы\n",
    "plot_correlation_matrix(corr_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно из графика выше, некоторые признаки сильно скоррелированы друг с другом. \n",
    "\n",
    "<font size=3, color=\"green\">__Задание №5.1: очистить лишнее__</font>\n",
    "\n",
    "Создайте 2 новые переменные __X_train_new__ и __X_test_new__ из __X_train__ и __X_test__, которые будут содержать только некоррелированные признаки. И обновите веса.\n",
    "\n",
    "__Результат:__ обновленные значения метрик на train и test датасетах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = np.abs(corr_matrix)\n",
    "corr_var = [y for x, y in zip(*np.where(corr_matrix > 0.9)) if x != y and x < y] + [14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T06:48:46.352448Z",
     "start_time": "2019-10-18T06:48:46.338437Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_new = np.delete(X_train, list(corr_var), 1)\n",
    "X_test_new  = np.delete(X_test, list(corr_var), 1)\n",
    "\n",
    "# TESTS\n",
    "assert type(X_train_new) == np.ndarray, 'Возвращается не корректный тип'\n",
    "assert type(X_test_new) == np.ndarray, 'Возвращается не корректный тип'\n",
    "assert X_test_new.shape[1] == X_train_new.shape[1], 'Количество признаков в train и test различно'\n",
    "assert X_test_new.shape[1] == 10, 'Количество признаков неверно'\n",
    "\n",
    "\n",
    "# Обновление весов\n",
    "W = least_squares_weights(X_train_new[:,:], Y_train)\n",
    "\n",
    "Y_train_predict = least_squares_predict(W, X_train_new[:,:])\n",
    "Y_test_predict  = least_squares_predict(W, X_test_new[:,:])\n",
    "\n",
    "# TESTS\n",
    "assert len(W) == 10, 'Количество весов неверно'\n",
    "assert R_square(Y_train, Y_train_predict) > .98, 'Что-то пошло не так, метрики должны быть выше'\n",
    "assert R_square(Y_test, Y_test_predict) > .98, 'Что-то пошло не так, метрики должны быть выше'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T-test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Вектор оценок весов $w$ является случайной величиной.__ \n",
    "Данный факт следует из аналитического решения $\\left({X}^{\\text{T}} {X}\\right)^{-1} {X}^{\\text{T}} {y}$, где $y$ так же случайная величина.\n",
    "\n",
    "Вывод этого утверждения довольно неприятен, и был разобран на лекции. Сейчас важно, что оценки коэффициентов $w_i$ имеют асимптотически нормальное распределение. \n",
    "\n",
    "------\n",
    "\n",
    "Данное свойство позволяет проверять гипотезу о равенстве коэффициента нулю.\n",
    "\n",
    "$$H_0: w_i=0 \\\\ H_1: w_i\\not=0$$\n",
    "\n",
    "__Статистика $t$__ имеет [распределение Стьюдента](https://ru.wikipedia.org/wiki/%D0%A0%D0%B0%D1%81%D0%BF%D1%80%D0%B5%D0%B4%D0%B5%D0%BB%D0%B5%D0%BD%D0%B8%D0%B5_%D0%A1%D1%82%D1%8C%D1%8E%D0%B4%D0%B5%D0%BD%D1%82%D0%B0) с $n-k$ степенями свободы при справедливости $H_0$:\n",
    "\n",
    "$$t=\\frac{w_i}{se(w_i)}$$\n",
    "\n",
    "__где__, \n",
    "\n",
    "- $n$ $-$ число объектов\n",
    "- $k$ $-$ число признаков\n",
    "- $se(w_i)$ $-$ стандартная ошибка $w_i$\n",
    "\n",
    "значения $se(w_i)$ - корень из диагональных элементов ковариационной матрицы $\\hat{Var}(w|X)$\n",
    "\n",
    "$$\\hat{Var}(w|X)=\\hat\\sigma^2({X}^{\\text{T}} {X})^{-1}$$\n",
    "\n",
    "__где__, $\\hat\\sigma^2$ является оценкой истинного значиния $\\sigma^2$ и вычисляется\n",
    "\n",
    "$$\\hat\\sigma^2=\\frac{RSS}{n-k}=\\frac{\\sum_{i=1}^n(y_i-wx_i)^2}{n-k}$$\n",
    "\n",
    "Таким образом, мы получили одновыборочный t-критерий Стьюдента. $\\text{p_value}$ в данном случае вычисляется как:\n",
    "\n",
    "$$\\text{p_value}=2(1-cdf(t, df=n-k))$$\n",
    "\n",
    "__где__,\n",
    "$cdf(t, df=n-k)$ $-$ __значение функции распределения__ Стьюдента c $n-k$ степенями свободы в точке $t$.\n",
    "\n",
    "________________\n",
    "\n",
    "Большинство реализаций данного метода сами расчитывают значения $t$-статистики и $\\text{p_value}$. Пример ниже:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T06:48:47.996485Z",
     "start_time": "2019-10-18T06:48:47.938519Z"
    }
   },
   "outputs": [],
   "source": [
    "from statsmodels.regression.linear_model import OLS\n",
    "model = OLS(Y_train,X_train_new[:,:], random_seed = 2)\n",
    "results = model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T06:48:48.290329Z",
     "start_time": "2019-10-18T06:48:48.284319Z"
    }
   },
   "outputs": [],
   "source": [
    "print('W OLS ', [round(i, 3) for i in results.params])\n",
    "print('t-stat', [round(i, 3) for i in results.tvalues])\n",
    "print('pvalue', [round(i, 3) for i in results.pvalues])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T07:37:22.645870Z",
     "start_time": "2019-10-16T07:37:22.634870Z"
    }
   },
   "source": [
    "Можете сверить значение весов, которые вы получили в предудыщих заданиях, с результатом работы матода OLS из пакета statsmodels. Если с точностью до 3-го знака все хорошо, то двигайся дальше, если нет, то что-то пошло не так.\n",
    "\n",
    "<font size=3, color=\"green\">__Задание №6: $\\text{p_value}$__</font>\n",
    "\n",
    "__В данном задании вам необходимо реализовать 3 функции и дать ответ на один вопрос:__\n",
    "- __variance_matrix__: расчет ковариационной матрицы. \n",
    "    - _Вход_: признаковое пространство $X$, истинные значения меток $\\text{y_true}$ и предсказанные значения $\\text{y_predict}$\n",
    "    - _Выход_: ковариационная матрица\n",
    "    \n",
    "    \n",
    "- __t_values__: расчет $t$-статистики\n",
    "    - _Вход_: массив весов $w$ и ковариационная матрица $\\text{X_variance}$\n",
    "    - _выход_: массив значений $t$-статистики для каждого из весов $w$\n",
    "    \n",
    "    \n",
    "- __p_values__: расчет $\\text{p_value}$\n",
    "    - _Вход_: массив значений $t$-статистики для каждого из весов $w$ и количество степеней свободы $df$\n",
    "    - _выход_: массив значений $\\text{p_value}$-статистики для каждого из весов $w$\n",
    "    \n",
    "    \n",
    "- Для скольких весов отвергается $H_0$ на уровне значимости $\\alpha=0.05$?\n",
    "    - Первый кто реализует все функции и скажет верный ответ, получит дополнительные баллы\n",
    "    \n",
    "__!!ВАЖНО!!__\n",
    "\n",
    "- Задание считается решенным, если все три функции работают корректно\n",
    "- Не используйте в этих функциях лишние циклы,  иначе они будут вычислительно неэффективными"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T06:48:49.586625Z",
     "start_time": "2019-10-18T06:48:49.575633Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def variance_matrix(X, y_true, y_predict):\n",
    "    sigma = ((y_true - y_predict) ** 2).sum() / (X.shape[0] - X.shape[1])\n",
    "    return np.linalg.inv(np.matmul(X.T, X)) * sigma\n",
    "\n",
    "vcov = variance_matrix(X_train_new[:,:], Y_train, Y_train_predict)\n",
    "\n",
    "#TESTS\n",
    "assert type(vcov) == np.ndarray, 'Возвращается не корректный тип'\n",
    "assert vcov.shape[0] == vcov.shape[1], 'Матрица не квадратная'\n",
    "assert vcov.shape[0] == W.shape[0], 'Матрица не верного размера'\n",
    "assert np.round(sum(map(lambda n: vcov[n,n], range(0, vcov.shape[0]))), 5) == 0.00011, 'Матрица рассчитана не корректно'\n",
    "# проверка на необходимые библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T06:48:50.086347Z",
     "start_time": "2019-10-18T06:48:50.081341Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def t_values(w, X_variance):\n",
    "    return w / np.sqrt(np.diagonal(X_variance))\n",
    "\n",
    "t = t_values(W, vcov) \n",
    "\n",
    "assert type(t) == np.ndarray, 'Возвращается не корректный тип'\n",
    "assert t.shape == W.shape, 'Размерность массива весов и массива t-статистик не совпадает'\n",
    "assert [round(i, 3) for i in t] == [round(i, 3) for i in results.tvalues], 'Результат рассчитан некорректно'\n",
    "# проверка на необходимые билиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T06:48:50.653035Z",
     "start_time": "2019-10-18T06:48:50.644030Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def p_values(t, df):\n",
    "    return 2 * (1 - stats.t.cdf(np.abs(t), df))\n",
    "\n",
    "p = p_values(t, X_train_new.shape[0]-X_train_new.shape[1])\n",
    "\n",
    "assert type(p) == np.ndarray, 'Возвращается не корректный тип'\n",
    "assert p.shape == W.shape, 'Размерность массива весов и массива p_value не совпадает'\n",
    "assert [round(i, 3) for i in p] == [round(i, 3) for i in results.pvalues], 'Результат рассчитан некорректно'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Для скольких весов отвергается H0 на уровне значимости alpha=0.05\n",
    "p[p < 0.05].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>__Немного теории__</font>\n",
    "\n",
    "У классического аналитического решения есть ряд недостатков:\n",
    "1. расчет псевдообратной матрицы является довольно сложной операцией и работает долго на больших объемах данных\n",
    "2. данный метод требует выполнения уловий [теоремы Гаусса - Маркова](https://ru.wikipedia.org/wiki/Теорема_Гаусса_—_Маркова) иначе оценки будут смещенными.\n",
    "\n",
    "__Градиентный спуск__\n",
    "\n",
    "Способ решить задачу численно. Нивелирует недостатки аналитического решения, однако не дает возможности вычислять значимость весов\\коэффициентов регрессии.\n",
    "\n",
    "В общем случае в процессе обучения, с помощью градиентного спуска, веса обновляются следующим образом:\n",
    "\n",
    "$$w^{n+1}_i=w^{n}_i-\\gamma\\bigtriangledown L(w^{n}_i)$$\n",
    "\n",
    "__где__,\n",
    "\n",
    "- $n$ $-$ индекc итерации\n",
    "- $i$ $-$ индекc признака\n",
    "- $\\gamma$ $-$ величина шага градиентного спуска\n",
    "- $\\bigtriangledown F(w^{n}_i)$ $-$ значение [градиентда](https://ru.wikipedia.org/wiki/%D0%93%D1%80%D0%B0%D0%B4%D0%B8%D0%B5%D0%BD%D1%82) функции потерь для $i$-го веса на шаге $n$\n",
    "\n",
    "__Недостаток градиентного спуска:__ На каждой итерации $n$ необходимо рассчитывать значение градиента для каждого объекта выборки. __Стохастический градиентный спуск__ является альтернативой. Единственное отличительное свойство: на каждой итерации градиент рассчитывается только на одном объекте, иногда на нескольких объектах.\n",
    "\n",
    "<font size=3, color=\"green\">__Задание №:7 Градиентный спуск__</font>\n",
    "\n",
    "__Цель:__ Релизовать метод стахостического градиентного спуска для решения задачи линейной регрессии с квадратичной функцией потерь и отследить процесс обучения с помощью визуализации.\n",
    "\n",
    "$$ L(y,X,w)=\\frac{1}{n} \\sum_{i=1}^n (y_i-Xw)^2$$\n",
    "\n",
    "Для решения этой задачи понадобится две функции, одна из которых уже полностью реализована\n",
    "\n",
    "функция __fit уже реализована__ и отвечает за:\n",
    "1. первичную инициализацию весов\n",
    "2. итерационное обновление весов (__iters__ - количество итераций)\n",
    "    - семплирует __batch_size__ элементов из исходного множества объектов. Обратите внимание, что объектов > 1 на каждой итерации\n",
    "    - вызывает функцию расчета градиента для заданных объектов (__stochastic_gradient_step__)\n",
    "3. визуализацию обучения (__plot__=True)\n",
    "\n",
    "__Вход__: матрица объект-признак $X$ и истинные значения $y$; значения весов, полученных аналитическим методом  $\\text{analytical_w}$\n",
    "\n",
    "__Выход__: значения весов $w$ на последней итерации стохастического градиентного спуска и значения весов при аналитическом решении\n",
    "_________\n",
    "\n",
    "__Вам необходимо:__ реализовать функцию __stochastic_gradient_step__, цель которой рассчитать значение градиента по каждому из весов на заданных объектах.\n",
    "\n",
    "__Вход:__\n",
    "\n",
    "- $w$ $-$ значения весов с предыдущего шага\n",
    "- $X$ $-$ матрица объект-признак, размера ($\\text{batch_size}$, $N$). $N$ $-$ число признаков\n",
    "- $y$ $-$ вектор истинных значений $y$ длины $\\text{batch_size}$\n",
    "\n",
    "__Выход:__ вектор значений градиента для каждого из весов\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T07:35:22.719660Z",
     "start_time": "2019-10-18T07:35:22.714649Z"
    },
    "code_folding": [
     9
    ]
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "def stochastic_gradient_step(w, X, y):\n",
    "    y_pred = X @ w\n",
    "    a = X.T @ (y_pred - y)\n",
    "    return (X.T @ (y_pred - y)) / X.shape[1]\n",
    "\n",
    "\n",
    "def fit(X, y, analytical_w, random_state=42, iters=100, batch_size=10, learning_rate=0.05, plot=True):\n",
    "    random_gen = np.random.RandomState(random_state)\n",
    "    errors = []\n",
    "    \n",
    "    \n",
    "    # получаем размерности матрицы\n",
    "    size, dim = X.shape\n",
    "\n",
    "    # случайная начальная инициализация\n",
    "    w = random_gen.rand(dim)\n",
    "    \n",
    "    weights_history = w\n",
    "    colors = [plt.get_cmap('gist_rainbow')(i) for i in np.linspace(0,1,dim)]\n",
    "    \n",
    "    # plt \n",
    "    if plot:\n",
    "        fig = plt.figure(figsize=(15, 10))\n",
    "        ax1 = fig.add_subplot(221)\n",
    "        ax2 = fig.add_subplot(222)\n",
    "        ax3 = fig.add_subplot(212)\n",
    "        fig.suptitle('Gradient descent')\n",
    "        \n",
    "        \n",
    "    for _ in range(iters):  \n",
    "        # берём случайный набор элементов\n",
    "        rand_indices = random_gen.choice(size, batch_size)\n",
    "        X_ = X[rand_indices]\n",
    "        y_ = y[rand_indices]\n",
    "\n",
    "        # считаем производные\n",
    "        grad = stochastic_gradient_step(w, X_, y_)\n",
    "        assert type(grad) == np.ndarray, 'неверный тип'\n",
    "        assert len(grad.shape) == 1, 'Необходимо вернуть одномерный вектор'\n",
    "        assert grad.shape[0] == len(w), 'длина вектора должна быть равной количеству весов'\n",
    "        \n",
    "        w   -= grad * learning_rate\n",
    "        \n",
    "        # Обновляем веса\n",
    "        weights_history = np.vstack((weights_history, w))\n",
    "        \n",
    "        # error\n",
    "        predict = least_squares_predict(w, X)\n",
    "        errors.append(R_square(y, predict))\n",
    "        \n",
    "        if plot:\n",
    "            ax1.clear()            \n",
    "            ax1.plot(range(dim), analytical_w, 'bo-', label='Analytical solution')\n",
    "            ax1.plot(range(dim), w, 'ro-', label='Gradient solution')\n",
    "            ax1.legend(loc=\"upper left\")\n",
    "            ax1.set_title('weights')\n",
    "            ax1.set_ylabel(r'$\\bar \\beta$')\n",
    "            ax1.set_xlabel('weight ID')\n",
    "            \n",
    "            \n",
    "            ax2.plot(range(_+1), errors, 'g-')\n",
    "            ax2.set_title('R_square')\n",
    "            ax2.set_xlabel('itarations')\n",
    "            \n",
    "            ax3.plot(weights_history)\n",
    "            ax3.set_title('update weights')\n",
    "            ax3.set_ylabel('value')\n",
    "            ax3.set_xlabel('itarations')\n",
    "            \n",
    "            time.sleep(0.3)\n",
    "            fig.canvas.draw()   \n",
    "            \n",
    "    return w, analytical_w\n",
    "\n",
    "# X_train_new матрица объект-признаки, полученная после выполнения задания №5.1:\n",
    "# должно быть 10 признаков\n",
    "\n",
    "w_SGD, w_OLS = fit(X_train_new, Y_train, analytical_w=W, plot=True)\n",
    "\n",
    "Y_test_predict_OLS  = least_squares_predict(w_OLS, X_test_new[:,:])\n",
    "Y_test_predict_SGD  = least_squares_predict(w_SGD, X_test_new[:,:])\n",
    "\n",
    "\n",
    "assert .98 <= R_square(Y_test, Y_test_predict_OLS)/R_square(Y_test, Y_test_predict_SGD) <= 1.02, 'задание решено не верно'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>__Немного теории__</font>\n",
    "\n",
    "Решение методом градиентного спуска имеет ряд недостатков:\n",
    "1.\tРешение получается не стабильным и переобученным в случае наличие коррелированных признаков. Градиент стремится завышать веса таких признаков, «игнорируя» другие признаки\n",
    "2.\tДанный метод не предполагает наличие незначимых признаков и стремиться выдать им хоть какой-то вес тем самым модель становиться менее устойчивой \n",
    "\n",
    "Решить эти проблемы можно с помощью регуляризаторов:\n",
    "- __$L2$ Rige regression.__ Штрафует за сильно высокие веса, срамясь уменьшить эффект 1-ой проблемы.\n",
    "\n",
    "$$L(y,X,w)=\\frac{1}{n} \\sum_{i=1}^n (y_i-Xw)^2+\\delta\\sum_{j=1}^pw_i^2$$ \n",
    "где $n$ $-$ количество объектов; $p$ $-$ количество признаков; $\\lambda$ $-$ коэффициент регуляризации\n",
    "\n",
    "- __$L1$ Lasso.__ Стремится обнулить наибольшее количество весов. Таким образом решается 2-ая проблема\n",
    "\n",
    "$$L(y,X,w)=\\frac{1}{n} \\sum_{i=1}^n (y_i-Xw)^2+\\lambda\\sum_{j=1}^p|w_i|$$ \n",
    "\n",
    "где $n$ $-$ количество объектов; $p$ $-$ количество признаков; $\\delta$ $-$ коэффициент регуляризации\n",
    "\n",
    "- __ElasticNet.__ Обобщенная форма $L1$ и $L2$\n",
    "\n",
    "$$L(y,X,w)=\\frac{1}{n} \\sum_{i=1}^n (y_i-Xw)^2+\\alpha\\gamma\\sum_{j=1}^p|w_i|+0.5\\alpha(1-\\gamma)\\sum_{j=1}^pw_i^2$$ \n",
    "\n",
    "Если при условии простого суммирования $\\lambda\\text{L1}+\\delta\\text{L2}$, то\n",
    "\n",
    "$\\alpha=\\lambda+\\delta$ $-$ общий множитель регуляризаторов. При $\\alpha=0$ получаем исходную функцию потерь, без штрафов. \n",
    "\n",
    "$\\gamma=\\frac{\\lambda}{\\lambda+\\delta}$ $-$ коэфициент вклада $L1$. Принимает значения от 0 до 1. При $\\gamma=0 \\rightarrow \\text{ElasticNet}=L2$; $\\gamma=1 \\rightarrow \\text{ElasticNet}=L1$\n",
    "\n",
    "<font size=3, color=\"green\">__Задание №:8 Градиентный спуск и регуляризация__</font>\n",
    "\n",
    "Вам необходимо __переаисать__ функцию __stochastic_gradient_step__ что бы она расчитывала значения градиентов для функции потерь __ElasticNet__.\n",
    "\n",
    "__Вход:__   \n",
    "- аналогично заданию №7\n",
    "- alpha $(\\alpha)$ $-$ общий множитель регуляризаторов \n",
    "- l1_ratio $(\\gamma)$ $-$ коэфициент вклада $L1$\n",
    "\n",
    "__Выход:__ вектор значений градиента для каждого из весов\n",
    "\n",
    "__!!ВАЖНО!!__: \n",
    "\n",
    "- Посмотрите на то как меняются веса при изменении параметра $\\text{l1_ratio}$\n",
    "- Функция __fit__ останется прежней"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T07:36:14.703686Z",
     "start_time": "2019-10-18T07:36:14.701680Z"
    }
   },
   "outputs": [],
   "source": [
    "# Изменить код необходимы функций так что бы можно использовать как l1 так и l2 регуляризацию одновременно\n",
    "# Как изменяются веса в краевых случаях\n",
    "def stochastic_gradient_step(w, X, y, alpha=1, l1_ratio=0.5):\n",
    "    y_pred = X @ w\n",
    "    a = X.T @ (y_pred - y)\n",
    "    gradient = 2 * ((X.T @ (y_pred - y)) / X.shape[1]) + alpha * l1_ratio * np.sign(w) + alpha * (1 - l1_ratio) * w\n",
    "    return gradient\n",
    "\n",
    "# Будем обучаться на исходном множестве признаков (14)\n",
    "# a_w - веса полученные аналитическим решением. Уменьшил некоторые сознательно, что бы график обучения был читабелен\n",
    "a_w = [-8.21, 12.21, 7.38, 29.11, 34.44, 62.33, 12.95, 149.06, 91.42, 99.11, -0.0, -20.72, -41.53, 86.23, 1]\n",
    "w = fit(X_train, Y_train, analytical_w=a_w)\n",
    "\n",
    "# проверка!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T06:49:36.849018Z",
     "start_time": "2019-10-18T06:49:36.844039Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T06:49:46.021853Z",
     "start_time": "2019-10-18T06:49:45.953890Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, Y_train, X_test, Y_test = read_task_2_3('./task_3.csv', bias=False)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T06:49:52.186725Z",
     "start_time": "2019-10-18T06:49:51.732989Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 10))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.title('train set')\n",
    "plt.scatter(X_train[:,0], X_train[:,1], s=40, c=Y_train)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.title('test set')\n",
    "plt.scatter(X_test[:,0], X_test[:,1], s=40, c=Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3, color=\"green\">__Задание №:9 Логистическая регрессия__</font>\n",
    "\n",
    "__Необходимо реазлизовать 2 функции:__\n",
    "\n",
    "- __probability__ отвечающая за реализацию логистической функции потерь\n",
    "\n",
    "$$L(X,w)=\\frac{1}{1+\\exp^{(-Xw)}}$$\n",
    "\n",
    "__Вход:__\n",
    "\n",
    "- $w$ $-$ значения весов\n",
    "- $X$ $-$ матрица объект-признак\n",
    "\n",
    "__Выход:__ массив значений функции потерь, длины $N$. Где $N$ $-$ количество объектов\n",
    "\n",
    "__________\n",
    "- __binary_class_prediction__ вызывает функцию __probability__ и на основе ее результата присваивает метки класса по следующей логике: if prob>threshold then 1 else 0\n",
    "\n",
    "__Выход:__ массив предсказанных меток класса\n",
    "\n",
    "\n",
    "__!!Важно!!:__ Веса получаем аналитическим методом для из заданий с регрессией\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = least_squares_weights(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T07:12:54.484788Z",
     "start_time": "2019-10-18T07:12:54.477794Z"
    }
   },
   "outputs": [],
   "source": [
    "def probability(W, X):\n",
    "    result = 1 / (1 + np.exp(-(X @ W)))\n",
    "    return result\n",
    "\n",
    "prob = probability(W, X_train)\n",
    "\n",
    "#TESTS\n",
    "assert type(prob) == np.ndarray, 'Возвращается неверный тип'\n",
    "assert prob.shape == (X_train.shape[0],), 'Неверный размер массива'\n",
    "assert [round(i,3) for i in prob[[1, -1, 12]]] == [0.451, 0.624, 0.511], 'Функция считается неверно'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T07:24:29.360016Z",
     "start_time": "2019-10-18T07:24:29.353015Z"
    }
   },
   "outputs": [],
   "source": [
    "def binary_class_prediction(W, X, threshold =.5):\n",
    "    prob =  probability(W, X)\n",
    "    return np.where(prob >= threshold, 1, 0)\n",
    "\n",
    "Y_train_predict = binary_class_prediction(W, X_train)\n",
    "Y_test_predict = binary_class_prediction(W, X_test)\n",
    "\n",
    "#TESTS\n",
    "assert type(Y_train_predict) == np.ndarray, 'Возвращается неверный тип'\n",
    "assert Y_train_predict.shape == (X_train.shape[0],), 'Неверный размер массива'\n",
    "assert min(Y_train_predict) == 0, 'Функция считается неверно'\n",
    "assert max(Y_train_predict) == 1, 'Функция считается неверно'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T07:37:47.224781Z",
     "start_time": "2019-10-18T07:37:47.221784Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.title('train set')\n",
    "plt.scatter(X_train[:,0], X_train[:,1], s=40, c=Y_train)\n",
    "\n",
    "# create grid to evaluate model\n",
    "x_min, x_max = plt.xlim() \n",
    "y_min, y_max = plt.ylim() \n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, .01),\n",
    "                     np.arange(y_min, y_max, .01))\n",
    "Z = binary_class_prediction(W, np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.contour(xx, yy, Z, colors='r', linewidths=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3, color=\"green\">__Задание №:10 accuracy__</font>\n",
    "\n",
    "Реализуйте функцию __binary_accuracy__ рассчитывающую долю верно размеченных единиц\n",
    "\n",
    "__Вход:__\n",
    "\n",
    "- y_true $-$ истинные метки классов\n",
    "- y_predict $-$ предсказанные метки классов\n",
    "\n",
    "__Выход:__ значение от 0 до 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 920,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T07:37:42.492392Z",
     "start_time": "2019-10-18T07:37:42.488413Z"
    }
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Функция считается неверно",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-920-4ffbdea3c79e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m<=\u001b[0m\u001b[0mbinary_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train_predict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m<=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Результат может принимать значения от 0 до 1'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbinary_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train_predict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0.75\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Функция считается неверно'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbinary_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test_predict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0.71\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Функция считается неверно'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: Функция считается неверно"
     ]
    }
   ],
   "source": [
    "def binary_accuracy(y_true, y_predict):\n",
    "    true_positive = np.size(y_true[(y_true == y_predict) & (y_true == 1)])\n",
    "    true_negative = np.size(y_true[(y_true == y_predict) & (y_true == 0)])\n",
    "    false_positive = np.size(y_true[(y_true != y_predict) & (y_true == 1)])\n",
    "    false_negative = np.size(y_true[(y_true != y_predict) & (y_true == 0)])\n",
    "    return np.divide(true_positive + true_negative, true_positive + true_negative + false_positive + false_negative)\n",
    "\n",
    "assert type(binary_accuracy(Y_train, Y_train_predict)) == np.float64, 'Возвращается неверный тип' \n",
    "assert 0 <= binary_accuracy(Y_train, Y_train_predict) <= 1, 'Результат может принимать значения от 0 до 1'\n",
    "assert round(binary_accuracy(Y_train, Y_train_predict),2) == 0.75, 'Функция считается неверно'\n",
    "assert round(binary_accuracy(Y_test, Y_test_predict),2) == 0.71, 'Функция считается неверно'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
